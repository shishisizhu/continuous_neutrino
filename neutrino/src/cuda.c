/**
 * Neutrino Hook Driver, NVIDIA CUDA Implementation
*/

#include "common.h" // for common headers
#include <cuda.h>   // for cuda related definition

/**
 * Undefine some symbols updated to v2. These are some historical issue with
 * 32bit machine. Now NVIDIA update them to v2 for 64bit.
 */
#undef cuMemAlloc
#undef cuStreamGetCaptureInfo
#undef cuArray3DCreate
#undef cuArray3DGetDescriptor
#undef cuArrayCreate
#undef cuArrayGetDescriptor
#undef cuCtxCreate
#undef cuCtxDestroy
#undef cuCtxPopCurrent
#undef cuCtxPushCurrent
#undef cuDevicePrimaryCtxRelease
#undef cuDevicePrimaryCtxReset
#undef cuDevicePrimaryCtxSetFlags
#undef cuDeviceTotalMem
#undef cuEventDestroy
#undef cuGetProcAddress
#undef cuGraphAddKernelNode
#undef cuGraphExecKernelNodeSetParams
#undef cuGraphExecUpdate
#undef cuGraphicsResourceGetMappedPointer
#undef cuGraphicsResourceSetMapFlags
#undef cuGraphKernelNodeGetParams
#undef cuGraphKernelNodeSetParams
#undef cuIpcOpenMemHandle
#undef cuLinkAddData
#undef cuLinkAddFile
#undef cuLinkCreate
#undef cuMemAllocHost
#undef cuMemAllocPitch
#undef cuMemcpy2DAsync
#undef cuMemcpy2DUnaligned
#undef cuMemcpy2D
#undef cuMemcpy3DAsync
#undef cuMemcpy3D
#undef cuMemcpyAtoA
#undef cuMemcpyAtoD
#undef cuMemcpyAtoHAsync
#undef cuMemcpyAtoH
#undef cuMemcpyDtoA
#undef cuMemcpyDtoDAsync
#undef cuMemcpyDtoD
#undef cuMemcpyDtoHAsync
#undef cuMemcpyDtoH
#undef cuMemcpyHtoAAsync
#undef cuMemcpyHtoA
#undef cuMemcpyHtoDAsync
#undef cuMemcpyHtoD
#undef cuMemFree
#undef cuMemGetAddressRange
#undef cuMemGetInfo
#undef cuMemHostGetDevicePointer
#undef cuMemHostRegister
#undef cuMemsetD16
#undef cuMemsetD2D16
#undef cuMemsetD2D32
#undef cuMemsetD2D8
#undef cuMemsetD32
#undef cuMemsetD8
#undef cuModuleGetGlobal
#undef cuStreamBatchMemOp
#undef cuStreamBeginCapture
#undef cuStreamDestroy
#undef cuStreamWaitValue32
#undef cuStreamWaitValue64
#undef cuStreamWriteValue32
#undef cuStreamWriteValue64
#undef cuTexRefGetAddress
#undef cuTexRefSetAddress2D
#undef cuTexRefSetAddress

#define WARP_SIZE 32 // NVIDIA GPUs use 32 for WARP_SIZE
// used by benchmark mode
static CUdeviceptr benchmark_flush_mem = 0u; // aka NULL

// following functions are hooked for internal usage
CUresult (*real_cuModuleLoadData)(CUmodule*, const void*) = NULL;
CUresult (*real_cuModuleLoadDataEx)(CUmodule*, const void*, unsigned int, CUjit_option*, void**) = NULL;
CUresult (*real_cuModuleGetFunction)(CUfunction*, CUmodule, const char*) = NULL;
CUresult (*real_cuKernelGetFunction)(CUfunction*, CUkernel) = NULL;
CUresult (*real_cuLibraryGetKernel)(CUkernel*, CUlibrary, const char*) = NULL;
CUresult (*real_cuLibraryGetModule)(CUmodule*, CUlibrary) = NULL;
CUresult (*real_cuLibraryLoadData)(CUlibrary*, const void*, CUjit_option*, void**, unsigned int, CUlibraryOption*, void**, unsigned int) = NULL;
CUresult (*real_cuLaunchKernel)(CUfunction, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, CUstream, void**, void**) = NULL;
CUresult (*real_cuMemAlloc_v2)(CUdeviceptr*, size_t) = NULL;
CUresult (*real_cuMemFree_v2)(CUdeviceptr) = NULL;
CUresult (*real_cuModuleLoad)(CUmodule*, const char*) = NULL;
CUresult (*real_cuModuleLoadFatBinary)(CUmodule*, const void*) = NULL;
CUresult (*real_cuLaunchKernelEx)(const CUlaunchConfig*, CUfunction, void**, void**) = NULL;

// helper macro to check cuda error
#define CUDA_CHECK(cmd) do {                    \
	CUresult result = cmd;                      \
	if (result != CUDA_SUCCESS) {               \
		const char *msg;                        \
		real_cuGetErrorName(result, &msg);      \
		printf("Neutrino fail: %s:%d '%s'\n",   \
					 __FILE__, __LINE__, msg);  \
		exit(EXIT_FAILURE);                     \
	}                                           \
} while (0)

// include auto-generated signatures for unmodified functions
// @note signature.c will be auto generated by parse.py
#include "signature.c"

/**
 * Function to initialize the environment, including
 * * init the cuda driver module via dlopen
 * * init the file system as specified above
 * * init the hashmap for binaries and CUfunction
 * * init commonly used functions like real_cuModuleLoad...
 * 
 * @note this will be called only once when any hooked driver function is called
 */
static void init(void) {
    pthread_once(&mutex_is_initialized, mutex_init);
    // init() is critical section to be protected
    pthread_mutex_lock(&mutex);
    if (shared_lib != NULL) { // then it has been initialized by another
        pthread_mutex_unlock(&mutex);
        return;
    }
    common_init(); // init common modules
    // load hooked function of Neutrino
    real_cuModuleLoadData      = dlsym(shared_lib, "cuModuleLoadData");
    real_cuModuleLoadDataEx    = dlsym(shared_lib, "cuModuleLoadDataEx");
    real_cuModuleGetFunction   = dlsym(shared_lib, "cuModuleGetFunction");
    real_cuKernelGetFunction   = dlsym(shared_lib, "cuKernelGetFunction");
    real_cuLibraryGetKernel    = dlsym(shared_lib, "cuLibraryGetKernel");
    real_cuLibraryGetModule    = dlsym(shared_lib, "cuLibraryGetModule");
    real_cuLibraryLoadData     = dlsym(shared_lib, "cuLibraryLoadData");
    real_cuLaunchKernel        = dlsym(shared_lib, "cuLaunchKernel");
    real_cuMemAlloc_v2         = dlsym(shared_lib, "cuMemAlloc_v2");
    real_cuMemFree_v2          = dlsym(shared_lib, "cuMemFree_v2");
    real_cuModuleLoad          = dlsym(shared_lib, "cuModuleLoad");
    real_cuModuleLoadFatBinary = dlsym(shared_lib, "cuModuleLoadFatBinary");
    real_cuLaunchKernelEx      = dlsym(shared_lib, "cuLaunchKernelEx");
    init_unmodified(); // init unmodified functions, defined in signature.c
    CHECK_DL(); // checking if any dl error presented
    // initialzie the L2 Flush Memory if benchmark is enabled
    if (NEUTRINO_BENCHMARK) {
        fprintf(event_log, "[benchmark] ENABLED L2 Flush Size %ld\n", NEUTRINO_BENCHMARK_FLUSH_MEM_SIZE);
        real_cuMemAlloc_v2(&benchmark_flush_mem, NEUTRINO_BENCHMARK_FLUSH_MEM_SIZE);
    }
    fprintf(event_log, "[init] success\n"); 
    // leaving critical section, unlock
    pthread_mutex_unlock(&mutex);
    return;
}

/**
 * Module Management: cuModuleXXX
 * @see https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MODULE.html
 * 
 * This is to track the lowering pass from code (binary on disk) to CUfunction
 */

// Triton use this to load data
CUresult cuModuleLoadData(CUmodule* module, const void* image) {
    if (shared_lib == NULL) { init(); }
    
    // call the real function, after this, module will be valid
    CUresult result = real_cuModuleLoadData(module, image);

    fprintf(event_log, "[mod] cuModuleLoadData %d module %p image %p \n", result, *module, image);

    void*  managed;
    size_t size;
    if (get_managed_code_size(&managed, &size, image) != -1) {
        binmap_set(*module, managed, size, NULL); // name = NULL as we don't know it now
    }
    
    return result;
}

CUresult cuLibraryLoadData(CUlibrary* library, const void* code, CUjit_option* jitOptions, void** jitOptionsValues, unsigned int numJitOptions, CUlibraryOption* libraryOptions, void** libraryOptionValues, unsigned int numLibraryOptions) {
    if (shared_lib == NULL) { init(); }

    CUresult result = real_cuLibraryLoadData(library, code, jitOptions, jitOptionsValues, numJitOptions, libraryOptions, libraryOptionValues, numLibraryOptions);
    fprintf(event_log, "[mod] cuLibraryLoadData %d lib %p code %p\n", result, *library, code);

    // update to hashmap
    void*  managed;
    size_t size;
    if (get_managed_code_size(&managed, &size, code) != -1) {
        binmap_set(*library, managed, size, NULL); // name = NULL as we don't know it now
    }

    return result;
}

CUresult cuModuleLoadDataEx(CUmodule* module, const void* image, unsigned int numOptions, CUjit_option* options, void** optionValues) {
    if (shared_lib == NULL) { init(); }

    CUresult ret = real_cuModuleLoadDataEx(module, image, numOptions, options, optionValues);
    
    fprintf(event_log, "[mod] cuModuleLoadDataEx mod %p code %p\n", *module, image);

    void*  managed;
    size_t size;
    if (get_managed_code_size(&managed, &size, image) != -1) {
        binmap_set(*module, managed, size, NULL); // name = NULL as we don't know it now
    }

    return ret;
}

// JAX use this API, but they don't pass in fatbin but cubin, so a wrong API to use...
CUresult cuModuleLoadFatBinary(CUmodule* module, const void* fatCubin) {
    if (shared_lib == NULL) { init(); }

    CUresult result = real_cuModuleLoadFatBinary(module, fatCubin); // call the symbol

    fprintf(event_log, "[mod] cuModuleLoadFatBinary mod %p code %p\n", *module, fatCubin);

    void*  managed;
    size_t size;
    if (get_managed_code_size(&managed, &size, fatCubin) != -1) {
        binmap_set(*module, managed, size, NULL); // name = NULL as we don't know it now
    }
    return result;
}

// @todo handle the multiple function with different name problem
CUresult cuModuleGetFunction(CUfunction* hfunc, CUmodule hmod, const char* name) {
    if (shared_lib == NULL) { init(); }

    // first update the name
    size_t len = strlen(name);
    char* managed_name = malloc(len);
    memcpy(managed_name, name, len);
    
    // call real function
    CUresult result = real_cuModuleGetFunction(hfunc, hmod, name);

    fprintf(event_log, "[mod] cuModuleGetFunction func %p mod %p name %s\n", *hfunc, hmod, name);

    // then update the key from module to function
    if (binmap_update_name_key(hmod, *hfunc, managed_name) == -1) {
        fprintf(event_log, "[hash] cuModuleGetFunction failed-update %p %p %s\n", hmod, *hfunc, managed_name);
    }
    
    return result;
}

CUresult cuKernelGetFunction(CUfunction* pFunc, CUkernel kernel) {
    if (shared_lib == NULL) { init(); }

    CUresult result = real_cuKernelGetFunction(pFunc, kernel);

    fprintf(event_log, "[mod] cuKernelGetFunction %p %p\n", *pFunc, kernel);

    // then update the key from kernel to function
    if (binmap_update_key(kernel, *pFunc) == -1) {
        fprintf(event_log, "[hash] cuKernelGetFunction failed-update %p %p\n", kernel, *pFunc);
    }
    
    return result;
}

CUresult cuLibraryGetKernel(CUkernel* pKernel, CUlibrary library, const char* name) {
    if (shared_lib == NULL) { init(); }

    // first update the name
    size_t len = strlen(name);
    char* managed_name = malloc(len);
    memcpy(managed_name, name, len);

    CUresult result = real_cuLibraryGetKernel(pKernel, library, name);

    fprintf(event_log, "[mod] cuLibraryGetKernel kernel %p lib %p name %s\n", *pKernel, library, name);

    // then update the key from library to kernel
    if (binmap_update_name_key(library, *pKernel, managed_name) == -1) {
        fprintf(event_log, "[hash] cuLibraryGetKernel failed-update %p %p %s\n", library, *pKernel, managed_name);
    }

    return result;
}

CUresult cuLibraryGetModule(CUmodule* pMod, CUlibrary library) {
    if (shared_lib == NULL) { init(); }

    CUresult result = real_cuLibraryGetModule(pMod, library);

    fprintf(event_log, "[mod] cuLibraryGetModule %d mod %p lib %p\n", result, *pMod, library);

    // then update the key from library to kernel
    if (binmap_update_key(library, *pMod) == -1) {
        fprintf(event_log, "[hash] cuLibraryGetModule failed-update %p %p\n", library, *pMod);
    }

    return result;
}

/**
 * Execution Control, cuLaunchXXX and cuFuncXXX
 * @see https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC.html
 * 
 * aims at providing runtime probing support
 */

CUresult cuLaunchKernel(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream, void** kernelParams, void** extra) {
    if (shared_lib == NULL) { init(); }

    char* kernel_name;
    if(binmap_get_kernel_name(f, &kernel_name) == -1) {
        log_msg("[unknown kernel] null\n");
    }
    else {
        log_msg("[kernel] %s\n", kernel_name);
    }
    

    //Original mode, Directly exec Original Mode
    if (!exec_mode) {
        goto backup;
    }

    // for time measurement to understand the overhead of Neutrino
    CUevent start_event, end_event;
    CUDA_CHECK(real_cuEventCreate(&start_event, CU_EVENT_DEFAULT));
    CUDA_CHECK(real_cuEventCreate(&end_event,   CU_EVENT_DEFAULT));
    
    CUDA_CHECK(real_cuEventRecord(start_event, hStream)); // use the stream specified in param

    float prologue_time, kernel_time, epilogue_time; // time
    CUresult result;
    CUfunction probed, pruned; // countd is only used when DYNAMIC == True
    int n_param, n_probe; 
    int* probe_sizes; // size of probes
    int* probe_types; // type of probes
    bool succeed;         // jit status
    // @note for dynamic buffer, i.e., only when DYNAMIC=true
    CUfunction countd = NULL;
    int n_count = 0, count_size = 0; // count_size is used only when DYNAMIC == True

    // try obtain the kernel compiled or raise compilation process 
    // @note count and record is only valid if succeed == true
    if (funcmap_get((void*)f, &kernel_name, &n_param, &n_probe, &probe_sizes, &probe_types, &succeed, (void**)&probed, (void**)&pruned, (void**)&countd) == -1) {
        fprintf(event_log, "[exec] funcmap-not-find %p\n", f);
        fflush(event_log); 
        // here try to get binary  from binmap and start JIT compile
        size_t size;
        void* bin;
        if (binmap_get(f, &size, &kernel_name, &bin) == -1) { // not found the binary, fall back
            fprintf(event_log, "[probe] can't-find %p\n", f);
            funcmap_set(f,kernel_name, 0, 0, NULL, NULL, false, NULL, NULL, NULL); // set dummy with status FALSE
            goto backup;
        } else {
            fprintf(event_log, "[probe] find %p name %s bin %p size %zu\n", f, kernel_name, bin, size);
            fflush(event_log);
            // create a directory under the kernel directory with kernel_name
            // @note Linux has limit on directory length 255, replace it to sh1 so 20 char
            // @bugfix PyTorch kernel name usually is extremely long :(
            // @bugfix Triton autotune leads to a set of kernel with same name -> use counter to differentiate
            char *tmp = sha1(kernel_name);
            char *folder_name = (char*) malloc(5 + strlen(tmp));
            sprintf(folder_name, "%d_%s", kernel_idx, tmp);
            free(tmp);
            kernel_idx++;
            fprintf(event_log, "[probe] rename %s %s\n", kernel_name, folder_name);
            char* dir = malloc(strlen(KERNEL_DIR) + strlen(folder_name) + 10);
            sprintf(dir, "%s/%s", KERNEL_DIR, folder_name);
            if (mkdir(dir, 0755) == 0) { 
                fprintf(event_log, "[probe] mkdir %s\n", dir);
            } else {
                fprintf(event_log, "[probe] can't-mkdir %s\n", dir);
                funcmap_set(f,kernel_name, 0, 0, NULL, NULL, false, NULL, NULL, NULL); // set dummy with status FALSE
                goto backup;
            }
            // create original.bin and write the binary to it
            char* path = malloc(strlen(dir) + 15);
            sprintf(path, "%s/original.bin", dir);
            FILE* original_bin = fopen(path, "wb");
            if (original_bin == NULL) {
                fprintf(event_log, "[probe] can't-open %s\n", path);
                funcmap_set(f,kernel_name, 0, 0, NULL, NULL, false, NULL, NULL, NULL); // set dummy with status FALSE
                goto backup;
            }
            fwrite(bin, size, 1, original_bin);
            fclose(original_bin);
            fprintf(event_log, "[probe] write %s\n", path);
            // create subprocess to run process.py, be aware of multi-processing
            pid_t pid = fork();
            if (pid < 0) {
                fprintf(event_log, "[probe] can't-folk\n");
                funcmap_set(f,kernel_name, 0, 0, NULL, NULL, false, NULL, NULL, NULL); // set dummy with status FALSE
                goto backup;
            } else if (pid == 0) { // child process, run python process.py kernel name
                // python process.py <work_dir> <kernel_name>
                execlp(NEUTRINO_PYTHON, NEUTRINO_PYTHON, NEUTRINO_PROBING_PY, dir, kernel_name, NULL);
                exit(EXIT_FAILURE); // reach here only if exec error -> failure
            } else { // parent process, wait for child
                fprintf(event_log, "[probe] subproc %s %s %s %s\n", NEUTRINO_PYTHON, NEUTRINO_PROBING_PY, dir, kernel_name);
                int status;
                waitpid(pid, &status, 0);
                if (status != EXIT_SUCCESS) { 
                    fprintf(event_log, "[probe] python failed\n");
                    funcmap_set(f,kernel_name, 0, 0, NULL, NULL, false, NULL, NULL, NULL); // set dummy with status FALSE
                    goto backup; 
                } else {
                    fprintf(event_log, "[probe] python succeed\n");
                }
            }
            // read the kernel.info from file system
            sprintf(path, "%s/kernel.info", dir);
            char* kernel_info = readf(path, "r");
            // poor parser for kernel.info
            // @todo add checking alignment
            char* kernel_end = strchr(kernel_info, '\n');
            *kernel_end = '\0';
            kernel_name = kernel_info;
            char* start = kernel_end + 1;
            sscanf(start, " %d\n%d\n", &n_param, &n_probe);
            // read sizes and types of probe 
            probe_sizes = malloc(n_probe * sizeof(int));
            probe_types = malloc(n_probe * sizeof(int));
            char* strptr = strchr(strchr(start, '\n') + 1, '\n') + 1;
            for (int idx = 0; idx < n_probe; idx++) {
                sscanf(strptr, "%d,%d\n", &probe_types[idx], &probe_sizes[idx]);
                strptr = strchr(strptr, '\n') + 1;
            }
            // // @note read process hook, not yet checked
            // char* info_end = strchr(strptr, '\n');
            // *info_end = '\0';
            // callback = strptr;
            // here read the 
            fprintf(event_log, "[probe] read %s name %s n_param %d n_probe %d \n", path, kernel_name, n_param, n_probe);
            // load probed.bin -> for collecting runtime info
            sprintf(path, "%s/probed.bin", dir);
            void* probed_bin = readf(path, "rb");
            // load pruned.bin -> for benchmark
            sprintf(path, "%s/pruned.bin", dir);
            void* pruned_bin = readf(path, "rb");
            // then load the binary to module
            CUmodule probed_mod, pruned_mod;
            // then get function with the SAME name -> we distinguish via Module
            CUDA_CHECK(real_cuModuleLoadData(&probed_mod,  probed_bin));
            CUDA_CHECK(real_cuModuleGetFunction(&probed, probed_mod, kernel_name));
            CUDA_CHECK(real_cuModuleLoadData(&pruned_mod,  pruned_bin));
            CUDA_CHECK(real_cuModuleGetFunction(&pruned, pruned_mod, kernel_name));
            if (DYNAMIC) {
                sprintf(path, "%s/countd.bin", dir);
                void* countd_bin = readf(path, "rb");
                CUmodule countd_mod;
                CUDA_CHECK(real_cuModuleLoadData(&countd_mod,  countd_bin));
                CUDA_CHECK(real_cuModuleGetFunction(&countd, countd_mod, kernel_name));
            }
            // add record to hashmap to avoid re-compile 
            funcmap_set(f, kernel_name, n_param, n_probe, probe_sizes, probe_types, true, probed, pruned, countd);
            fprintf(event_log, "[probe] finish %p name %s n_param %d\n", f, kernel_name, n_param);
            fflush(event_log);
            // free memory before we leave
            free(dir);
            free(path);
            free(kernel_info);
            free(probed_bin);
            free(pruned_bin);
            free(folder_name);
            // don't free(probe_sizes) -> used by func-map!!!
            succeed = true;
        }
    }
    // expose the original param
    fprintf(event_log, "[exec] funcmap-find %p %s\n", f, succeed ? "success" : "fail");
    // check the jit status, if failed, goto backup
    if (!succeed) { goto backup; }

    // @bugfix add timestamp to match with readings from high-level integration (PyTorch)
    struct timespec ts;
    clock_gettime(CLOCK_REALTIME, &ts);
    long long time = ts.tv_nsec + ts.tv_sec * 1e9;
    fprintf(event_log, "[exec] %lld param ", time);
    for (int i = 0; i < n_param; i++) {
        // @note print raw value -> help check raw number but mostly pointers...
        fprintf(event_log, "%llx ", *(CUdeviceptr*)kernelParams[i]);
    } 
    fprintf(event_log, "\n");
    fprintf(event_log, "[exec] grid %u %u %u block %u %u %u shared %u\n", gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, sharedMemBytes);
    fflush(event_log);

    const size_t gridSize = gridDimX * gridDimY * gridDimZ;   
    const size_t blockSize = blockDimX * blockDimY * blockDimZ;
    const size_t warpSize = CDIV(blockSize, WARP_SIZE);
    
    /**
     * Handling Dynamic Memory Size Requirements
     * @bug  COUNT will not work for kernels with inplace modification, i.e., 
     *       the computed result will pollute next launch, usually seen in the
     *       decode kernels like topP/topK, please use a large enough constant
     * @note We may turns to a ring buffer implementation for dynamic buffers,
     *       but the trouble is how to have least interruption to frontned job
     *       Particularly under the case that PCIe speed <<< Memory Bandwidth
     * @todo Support Multiple Dynamic Buffer Allocation (by count many times)
     */
    if (DYNAMIC) { 
        n_count = 1; // Let's support 1 dynamic first
        // first set the attributes
        CUDA_CHECK(real_cuFuncSetAttribute(countd, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, sharedMemBytes));
        // second set the memory sizes
        const size_t size_counter = gridSize * blockSize * sizeof(uint64_t);
        uint64_t *h_counter = malloc(size_counter);
        CUdeviceptr d_counter;
        CUDA_CHECK(real_cuMemAlloc_v2(&d_counter, size_counter));
        CUDA_CHECK(real_cuMemsetD32_v2(d_counter, 0, size_counter / 4UL));
        void** count_args = malloc((n_param + 1) * sizeof(void*));
        memcpy(count_args, kernelParams, n_param * sizeof(void*)); // copy the raw parameters
        count_args[n_param] = &d_counter;
        result = real_cuLaunchKernel(countd, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, 
                                        blockDimZ, sharedMemBytes, hStream, count_args, extra);
        // cuMemcpy is a blocked call
        result = real_cuMemcpyDtoH_v2(h_counter, d_counter, size_counter);
        if (result != CUDA_SUCCESS) 
            goto backup;
        for (int idx = 0; idx < gridSize * blockSize; idx++) 
            count_size = (count_size > h_counter[idx]) ? count_size : h_counter[idx]; 
        fprintf(event_log, "[count] size: %d \n", count_size);
        // clean memory before we leave
        free(h_counter);
        free(count_args);
        CUDA_CHECK(real_cuMemFree_v2(d_counter));
    }

    // here start to calculate memory size for every probe based on grid, block and probe_sizes
    // formula similar to ndarray based on grid, block / warp
    
    size_t *probe_real_sizes = malloc(n_probe * sizeof(size_t));
    size_t total_probe_sizes = 0;
    for (int idx = 0; idx < n_probe; idx++) {
        if (probe_types[idx] == PROBE_TYPE_THREAD) {
            if (probe_sizes[idx] != -1) {
                probe_real_sizes[idx] = gridSize * blockSize * probe_sizes[idx];
                fprintf(event_log, "[exec] grid %zu block %zu probe %d total %zu\n", gridSize, blockSize, probe_sizes[idx], probe_real_sizes[idx]);
            } else {
                probe_real_sizes[idx] = gridSize * blockSize * count_size;
                fprintf(event_log, "[exec] grid %zu block %zu probe %d total %zu\n", gridSize, blockSize, count_size, probe_real_sizes[idx]);
            }
        } else if (probe_types[idx] == PROBE_TYPE_WARP) {
            probe_real_sizes[idx] = gridSize * warpSize * probe_sizes[idx];
            fprintf(event_log, "[exec] grid %zu warp  %zu probe %d total %zu\n", gridSize, warpSize, probe_sizes[idx], probe_real_sizes[idx]);
        }
        total_probe_sizes += probe_real_sizes[idx];
    }

    fprintf(event_log, "[exec] probe-mem %zu (bytes)\n", total_probe_sizes);

    // if NEUTRINO_MEMUSAGE, don't execute, just leave
    if (NEUTRINO_MEMUSAGE) {
        free(probe_real_sizes); // free the allocated
        goto backup; 
    }
    
    // Allocate Memory on Host and Device
    void** h_probe_mems = malloc(n_probe * sizeof(void*));
    CUdeviceptr* d_probe_mems = malloc(n_probe * sizeof(CUdeviceptr));
    for (int idx = 0; idx < n_probe; idx++) {
        h_probe_mems[idx] = malloc(probe_real_sizes[idx]);
        CUDA_CHECK(real_cuMemAlloc_v2(&d_probe_mems[idx], probe_real_sizes[idx]));
        CUDA_CHECK(real_cuMemsetD32_v2(d_probe_mems[idx], 0, probe_real_sizes[idx] / 4UL));
    }
    
    // @note argument layout is (n_param + n_probe) * sizeof(void*), n_param is parsed inside ptx
    void** probe_args = malloc((n_param + n_probe + n_count) * sizeof(void*));
    // first copy the raw parameters
    memcpy(probe_args, kernelParams, n_param * sizeof(void*)); 
    for (int idx = 0; idx < n_probe; idx++) { 
        probe_args[n_param + idx] = &d_probe_mems[idx]; // offset with n_param -> place later
    }
    for (int idx = 0; idx < n_count; idx++) {
        probe_args[n_param + n_probe + idx] = &count_size; // similar offset
    }
    
    /**
     * @note set the shared memory size. If the kernel shared memory size exceed a limit (usually half) 
     * of the physical SMEM size (per SM), then cuLaunchKernel will raise CUDA_ERROR_INVALID_VALUE, we
     * need to manually set via cuFuncSetAttribute(kernel, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared)
     * 
     * @details Neutrino JIT Function is considered a new one and can not inherit original setup...
     */
    CUDA_CHECK(real_cuFuncSetAttribute(probed, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, sharedMemBytes));
    if (NEUTRINO_BENCHMARK) {
        real_cuMemsetD32_v2(benchmark_flush_mem,0, NEUTRINO_BENCHMARK_FLUSH_MEM_SIZE / 4UL);
    }
    CUDA_CHECK(real_cuEventRecord(end_event, hStream)); // use the stream specified in param
    CUDA_CHECK(real_cuEventSynchronize(end_event));
    CUDA_CHECK(real_cuEventElapsedTime(&prologue_time, start_event, end_event));
    CUDA_CHECK(real_cuEventRecord(start_event, hStream)); // use the stream specified in param
    // launch kernel by call real_cuLaunchKernel function
    result = real_cuLaunchKernel(probed, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, 
                                    blockDimZ, sharedMemBytes, hStream, probe_args, extra);
    CUDA_CHECK(real_cuEventRecord(end_event, hStream)); // use the stream specified in param
    CUDA_CHECK(real_cuEventSynchronize(end_event));     // kernel ends at this line
    // calculate the real kernel time
    CUDA_CHECK(real_cuEventElapsedTime(&kernel_time, start_event, end_event));
    CUDA_CHECK(real_cuEventRecord(start_event, hStream)); // use the stream specified in param
    if (result != CUDA_SUCCESS) {
        for (int idx = 0; idx < n_probe; idx++) {
            free(h_probe_mems[idx]);
            CUDA_CHECK(real_cuMemFree_v2(d_probe_mems[idx]));
        }
        free(h_probe_mems);
        free(d_probe_mems);
        free(probe_real_sizes);
        free(probe_args);
        fprintf(event_log, "[exec] failed %d\n", result);
        goto backup;
    } else {
        fprintf(event_log, "[exec] succeed %d\n", result);
    }

    // On benchmark, we don't save results because we're testing the kernel
    if (NEUTRINO_BENCHMARK) { 
        goto leave; 
    }

    /**
     * Saving Trace to disk
     * @todo Standardize this part in common.h because it's platform-agnostic!
     */
    for (int idx = 0; idx < n_probe; idx++) {
        CUDA_CHECK(real_cuMemcpyDtoH_v2(h_probe_mems[idx], d_probe_mems[idx], probe_real_sizes[idx]));
    }
    // create dump file
    char* DUMP_FILE_NAME = malloc(strlen(RESULT_DIR) + 20);
    struct timespec end;
    clock_gettime(CLOCK_REALTIME, &end);
    double elapsed = ((end.tv_sec * 1e9 + end.tv_nsec) - (start.tv_sec * 1e9 + start.tv_nsec)) / 1e9;
    sprintf(DUMP_FILE_NAME, "%s/%.6f.bin", RESULT_DIR, elapsed);
    FILE *fp = fopen(DUMP_FILE_NAME, "wb");
    if (!fp) { 
        fprintf(event_log, "[exec] can't-save %s\n", DUMP_FILE_NAME); 
        return CUDA_SUCCESS; 
    }
    // write header to file
    trace_header_t header = { gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, sharedMemBytes, n_probe };
    fwrite(&header, sizeof(header), 1, fp);
    // write sections to file
    size_t offset = sizeof(header) + n_probe * sizeof(trace_section_t);
    for (int idx = 0; idx < n_probe; idx++) {
        trace_section_t section;
        section.size = probe_sizes[idx] != -1 ? probe_sizes[idx] : count_size; 
        section.warpDiv = (probe_types[idx] == PROBE_TYPE_WARP) ? WARP_SIZE : 1; 
        section.offset = offset;
        offset += gridSize * blockSize * section.size / section.warpDiv; 
        fwrite(&section, sizeof(section), 1, fp);
    }
    // write data
    for (int idx = 0; idx < n_probe; idx++) {
        fwrite(h_probe_mems[idx], 1, probe_real_sizes[idx], fp);
    }
    // close file
    fclose(fp);
    fprintf(event_log, "[exec] save %s size %zu\n", DUMP_FILE_NAME, offset);
    
leave:
    // on leave
    // free allocated memory before leave
    for (int idx = 0; idx < n_probe; idx++) {
        free(h_probe_mems[idx]);
        CUDA_CHECK(real_cuMemFree_v2(d_probe_mems[idx]));
    }
    CUDA_CHECK(real_cuEventRecord(end_event, hStream)); // use the stream specified in param
    CUDA_CHECK(real_cuEventSynchronize(end_event));
    CUDA_CHECK(real_cuEventElapsedTime(&epilogue_time, start_event, end_event));

    if (NEUTRINO_BENCHMARK)  { 
        // On benchmark mode, we 
        // @note it seems this will launch a kernel implicitly to clear L2 cache
        real_cuMemsetD32_v2(benchmark_flush_mem,0, NEUTRINO_BENCHMARK_FLUSH_MEM_SIZE / 4UL);
        // here Neutrino use pruned ptx being compiled with exactly the same configuration (assmbler & optimization) with probed
        float original_time;
        CUDA_CHECK(real_cuFuncSetAttribute(pruned, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, sharedMemBytes));
        CUDA_CHECK(real_cuEventRecord(start_event, hStream)); // use the stream specified in param
        // launch original kernel with original parameter
        result = real_cuLaunchKernel(pruned, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, sharedMemBytes, hStream, kernelParams, extra);
        CUDA_CHECK(real_cuEventRecord(end_event, hStream)); // use the stream specified in param
        CUDA_CHECK(real_cuEventSynchronize(end_event));
        // calculate the real kernel time
        CUDA_CHECK(real_cuEventElapsedTime(&original_time, start_event, end_event));
        fprintf(event_log, "[benchmark] prologue %f kernel %f epilogue %f original %f impact %f %d\n", prologue_time, kernel_time, epilogue_time, original_time, kernel_time / original_time, result);
    } else {
        // In normal mode, report the prologue, kernel, epilogue and impact ratio
        fprintf(event_log, "[exec] prologue %f kernel %f epilogue %f ratio %f\n", prologue_time, kernel_time, epilogue_time, (prologue_time + kernel_time + epilogue_time) / kernel_time);
        // Also create subprocess for analyze routi
        if (NEUTRINO_CALLBACK && strlen(NEUTRINO_CALLBACK) >= 3 && strcmp(NEUTRINO_CALLBACK + strlen(NEUTRINO_CALLBACK) - 3, ".py") == 0) {
            pid_t pid = fork();
            if (pid < 0) {
                fprintf(event_log, "[probe] can't-folk\n");
            } else if (pid == 0) { // child process, run python process.py kernel name
                // python process.py <work_dir> <kernel_name>
                execlp(NEUTRINO_PYTHON, NEUTRINO_PYTHON, NEUTRINO_CALLBACK, DUMP_FILE_NAME, NULL);
                exit(EXIT_FAILURE); // reach here only if exec error -> failure
            } else { // parent process, wait for child
                fprintf(event_log, "[callback] subproc %s %s %s\n", NEUTRINO_PYTHON, NEUTRINO_CALLBACK, DUMP_FILE_NAME);
                int status;
                waitpid(pid, &status, 0);
                if (status != EXIT_SUCCESS) { 
                    fprintf(event_log, "[callback] failed\n");
                } else {
                    fprintf(event_log, "[callback] succeed\n");
                }
            }
        }
        free(DUMP_FILE_NAME);
    }
    
    free(h_probe_mems);
    free(d_probe_mems);
    free(probe_real_sizes);
    free(probe_args);
    fflush(event_log);   // make sure all logs are written before we go
    return CUDA_SUCCESS; // reach here must be CUDA_SUCCESS

backup:
    result = real_cuLaunchKernel(f, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, sharedMemBytes, hStream, kernelParams, extra);
    return result;
}

/**
 * Memory API that helps debugging memory operation erros
 */
CUresult cuMemAlloc_v2(CUdeviceptr *dptr, size_t bytesize) {
    if (shared_lib == NULL) { init(); } 

    CUresult result = real_cuMemAlloc_v2(dptr, bytesize);
    fprintf(event_log, "[mem] cuMemAlloc_v2 %d dptr %llx bytesize %zu\n", result, *dptr, bytesize);
    return result;
}

CUresult cuMemFree_v2(CUdeviceptr dptr) {
    if (shared_lib == NULL) { init(); }

    CUresult result = real_cuMemFree_v2(dptr);
    fprintf(event_log, "[mem] cuMemFree_v2 %d dptr %llx\n", result, dptr);
    return result;
}

/**
 * Following functions shall also be hooked but we don't observe any workload
 * calling them, thus having a [info] section for tracing, add if needed
 */
CUresult cuModuleLoad(CUmodule* module, const char* fname) {
    if (shared_lib == NULL) { init(); }
    
    CUresult result = real_cuModuleLoad(module, fname); // call the symbol
    fprintf(event_log, "[info] cuModuleLoad %d\n", result);
    return result;
}


/**
 * Unmodified part of code, automatically generated by parse.py
 * usually we don't trace these API, just print a event_log to indicate they're used
 * if there's any weird behavior caused by Neutrino (unlikely), we can have a look
 */
#include "unmodified.c" // include the auto-generated code

/**
 * @note this function is intentially masked out by Neutrino because it might
 *       let process jump out of Neutrino's Hook Driver directly to real driver
 */
/*
CUresult cuGetProcAddress_v2(const char* symbol, void** pfn, int cudaVersion, cuuint64_t flags, CUdriverProcAddressQueryResult* symbolStatus) {
    if (real_cuGetProcAddress_v2 == NULL)
        init();
    
    CUresult ret;

    if (strcmp(symbol, "cuGetProcAddress") == 0) {
        CUresult (*cuGetProcAddress_v2_ptr)(const char* symbol, void** pfn, int cudaVersion, cuuint64_t flags, CUdriverProcAddressQueryResult* symbolStatus) = cuGetProcAddress_v2;
        *pfn = cuGetProcAddress_v2_ptr;
        ret = CUDA_SUCCESS;
        fprintf(event_log, "[pass] cuGetProcAddress_v2 %d %s %d return-myself\n", ret, symbol, cudaVersion); // unexpected func call
    } else if (strcmp(symbol, "cuGetExportTable") == 0) {
        // CUresult (*cuGetExportTable_ptr)(const void**, const CUuuid*) = cuGetExportTable;
        // *pfn = cuGetExportTable_ptr;
        ret = CUDA_ERROR_INVALID_VALUE;
        fprintf(event_log, "[pass] cuGetProcAddress_v2 %d %s %d return-ours\n", ret, symbol, cudaVersion); // unexpected func call
    } else {
        ret = real_cuGetProcAddress_v2(symbol, pfn, cudaVersion, flags, symbolStatus);
        fprintf(event_log, "[pass] cuGetProcAddress_v2 %d %s %d\n", ret, symbol, cudaVersion); // unexpected func call
    }
   
    return ret;
}
*/
